# Same as `8_bst_task1` but using custom normalization values

#### general settings
name: 15_bst_task1
use_tb_logger: true
model: SRFL_Glow
distortion: sr
scale: 8
gpu_ids: [0,] # should be equal to $CUDA_VISIBLE_DEVICES


#### datasets
datasets:
  train:
    name: train
    mode: LQGT_multiclass
    # use create_train_dataset.py to create downsampled datasets
    dataroot_GT: [../datasets/AIM-RWSR/train-clean-images/4x/,  # DIV2k clean 4x, (336x504)
                  ../datasets/BST/processed/task1/train-noisy-images/1x/]  # BST noisy 1x, (256x256)
    dataroot_LQ: [../datasets/AIM-RWSR/train-clean-images/32x/, # DIV2k clean 32x, (42x63)
                  ../datasets/BST/processed/task1/train-noisy-images/8x/]  # BST noisy 8x, (32x32)
    n_workers: 3  # per GPU
    preload: true # set true to load images into ram for faster training
    batch_size: 8
    balanced: true
    use_shuffle: true
    use_flip: true
    use_crop: true
    color: RGB
    GT_size: 160
    quant: 32
    alignment: 8
    normalize: # chanel-wise statistics over domains and lr/hr [values are precomputed]
      mean_noisy_hr: [117.88748611, 109.4048288, 97.64644714]
      std_noisy_hr: [70.1809656, 66.55224778, 69.63500642]
      mean_noisy_lr: [117.89141602, 109.40889648, 97.6509375 ]
      std_noisy_lr: [65.56253036, 62.15119827, 64.61089313]

      mean_clean_hr: [114.69150172, 111.67491841, 103.26490806]
      std_clean_hr: [70.6325031, 67.0257846, 72.95698077]
      mean_clean_lr: [114.69280773, 111.67589019, 103.26432346]
      std_clean_lr: [65.26659743, 61.88217135, 68.44474713]

  val:
    name: val
    mode: LQGT_multiclass
    dataroot_GT: [../datasets/BST/processed/task1/val-gt-clean/2x/,    # BST clean 2x
                  ../datasets/BST/processed/task1/val-input-noisy/1x/] # BST noisy 1x
    dataroot_LQ: [../datasets/BST/processed/task1/val-gt-clean/16x/,    # BST clean 8x
                  ../datasets/BST/processed/task1/val-input-noisy/8x/] # BST noisy 4x
    preload: true # set true to load images into ram for faster training
    center_crop_hr_size: 160
    quant: 32
    n_max: 5
    alignment: 8
    normalize:
      mean_noisy_hr: [117.88748611, 109.4048288, 97.64644714]
      std_noisy_hr: [70.1809656, 66.55224778, 69.63500642]
      mean_noisy_lr: [117.89141602, 109.40889648, 97.6509375 ]
      std_noisy_lr: [65.56253036, 62.15119827, 64.61089313]

      mean_clean_hr: [114.69150172, 111.67491841, 103.26490806]
      std_clean_hr: [70.6325031, 67.0257846, 72.95698077]
      mean_clean_lr: [114.69280773, 111.67589019, 103.26432346]
      std_clean_lr: [65.26659743, 61.88217135, 68.44474713]

#### network structures
network_G:
  which_model_G: RRDBGlowNet
  in_nc: 3
  out_nc: 3
  nf: 64
  nb: 23
  upscale: 8
  train_RRDB: true

  flow:
    K: 16
    L: 3
    noInitialInj: true
    LU_decomposed: true
    coupling: CondAffineSeparatedAndCond
    additionalFlowNoAffine: 2
    split:
      enable: true
    fea_up0: true
    stackRRDB:
      blocks: [1, 8, 15, 22]
      concat: true
    shift:
      constant: correlated
      classes: [[0,1],]
      std_init_shift: 0.0
    LR_noise_std: 0.03
    CondAffineSeparatedAndCond:
      eps: 0.001
      multReverse: True
      hidden_channels: 128

#### path
path:
  root: ../ # training results & checkpoints are saved in {root}/experiments
  pretrain_model_G: ../trained_models/RRDB_models/RRDB_PSNR_x8.pth
  strict_load: true
  resume_state: auto # starts training from last checkpoint if one exists

#### training settings: learning rate scheme, loss
train:
  manual_seed: 10
  lr_G: !!float 1.0e-4
  weight_decay_G: 0
  beta1: 0.9
  beta2: 0.99
  lr_scheme: MultiStepLR
  warmup_iter: -1  # no warm up
  lr_steps_rel: [0.5, 0.75, 0.9, 0.95]
  lr_gamma: 0.5

  niter: 100000
  val_freq: 20000

#### validation settings
val:
  heats: [1.0]
  n_sample: 1

#### logger
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
