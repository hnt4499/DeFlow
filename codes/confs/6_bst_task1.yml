# Similar to `2_bst_task1` but use training settings from DeFlow-AIM-RWSR, and use only
# one GPU, and use crop size of 64

#### general settings
name: 6_bst_task1
use_tb_logger: true
model: SRFL_Glow
distortion: sr
scale: 4
gpu_ids: [0,] # should be equal to $CUDA_VISIBLE_DEVICES


#### datasets
datasets:
  train:
    name: train
    mode: LQGT_multiclass
    # use create_train_dataset.py to create downsampled datasets
    dataroot_GT: [../datasets/BST/processed/task1/train-noisy-images/1x/,  # BST noisy 1x
                  ../datasets/BST/processed/task1/train-noisy-images/4x/]  # BST noisy 4x
    dataroot_LQ: [../datasets/BST/processed/task1/train-noisy-images/4x/,  # BST noisy 4x
                  ../datasets/BST/processed/task1/train-noisy-images/16x/] # BST noisy 16x
    n_workers: 3  # per GPU
    preload: true # set true to load images into ram for faster training
    batch_size: 8
    balanced: true
    use_shuffle: true
    use_flip: true
    use_crop: true
    color: RGB
    GT_size: 64
    quant: 32
    alignment: 8

  val:
    name: val
    mode: LQGT_multiclass
    dataroot_GT: [../datasets/BST/processed/task1/val-gt-clean/2x/,    # BST clean 2x
                  ../datasets/BST/processed/task1/val-input-noisy/1x/] # BST noisy 1x
    dataroot_LQ: [../datasets/BST/processed/task1/val-gt-clean/8x/,    # BST clean 8x
                  ../datasets/BST/processed/task1/val-input-noisy/4x/] # BST noisy 4x
    preload: true # set true to load images into ram for faster training
    center_crop_hr_size: 160
    quant: 32
    n_max:
    alignment: 8

#### network structures
network_G:
  which_model_G: RRDBGlowNet
  in_nc: 3
  out_nc: 3
  nf: 64
  nb: 23
  upscale: 4
  train_RRDB: true

  flow:
    K: 16
    L: 3
    noInitialInj: true
    LU_decomposed: true
    coupling: CondAffineSeparatedAndCond
    additionalFlowNoAffine: 2
    split:
      enable: true
    fea_up0: true
    stackRRDB:
      blocks: [1, 8, 15, 22]
      concat: true
    shift:
      constant: correlated
      classes: [[0,1],]
      std_init_shift: 0.0
    LR_noise_std: 0.03
    CondAffineSeparatedAndCond:
      eps: 0.001
      multReverse: True
      hidden_channels: 128

#### path
path:
  root: ../ # training results & checkpoints are saved in {root}/experiments
  pretrain_model_G: ../trained_models/RRDB_models/RRDB_PSNR_x4.pth
  strict_load: true
  resume_state: auto # starts training from last checkpoint if one exists

#### training settings: learning rate scheme, loss
train:
  manual_seed: 10
  lr_G: !!float 1.0e-4
  weight_decay_G: 0
  beta1: 0.9
  beta2: 0.99
  lr_scheme: MultiStepLR
  warmup_iter: -1  # no warm up
  lr_steps_rel: [0.5, 0.75, 0.9, 0.95]
  lr_gamma: 0.5

  niter: 100000
  val_freq: 20000

#### validation settings
val:
  heats: [1.0]
  n_sample: 1

#### logger
logger:
  print_freq: 100
  save_checkpoint_freq: !!float 5e3
